<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Chenzhou" />










<meta name="description" content="SVM 支持向量机简介 SVM 完整推导及SMO算法实现。 若干面试常见问题  参考  李航 统计学习方法 西瓜书">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM 支持向量机">
<meta property="og:url" content="http://yoursite.com/2018/10/13/SVM/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="SVM 支持向量机简介 SVM 完整推导及SMO算法实现。 若干面试常见问题  参考  李航 统计学习方法 西瓜书">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/10/13/SVM/linearsvm.png">
<meta property="og:image" content="http://yoursite.com/2018/10/13/SVM/kernal.png">
<meta property="og:image" content="http://yoursite.com/2018/10/13/SVM/softsvm-8314268.png">
<meta property="og:updated_time" content="2018-10-13T14:32:53.034Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SVM 支持向量机">
<meta name="twitter:description" content="SVM 支持向量机简介 SVM 完整推导及SMO算法实现。 若干面试常见问题  参考  李航 统计学习方法 西瓜书">
<meta name="twitter:image" content="http://yoursite.com/2018/10/13/SVM/linearsvm.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":false}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/13/SVM/"/>





  <title>SVM 支持向量机 | Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/13/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">SVM 支持向量机</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-13T22:13:59+08:00">
                2018-10-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="SVM-支持向量机"><a href="#SVM-支持向量机" class="headerlink" title="SVM 支持向量机"></a>SVM 支持向量机</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ol>
<li>SVM 完整推导及SMO算法实现。</li>
<li>若干面试常见问题</li>
</ol>
<p>参考</p>
<ol>
<li>李航 统计学习方法</li>
<li>西瓜书</li>
</ol>
<a id="more"></a>
<h2 id="1-线性svm"><a href="#1-线性svm" class="headerlink" title="1    线性svm"></a>1    线性svm</h2><h3 id="1-1-目标函数"><a href="#1-1-目标函数" class="headerlink" title="1.1     目标函数"></a>1.1     目标函数</h3><p>二维情况下的分类问题：</p>
<p><img src="/2018/10/13/SVM/linearsvm.png" width="400px" height="350px"></p>
<p>考虑多维情况的二分类问题，假设存在一个超平面能够将正负样本划分开，则所有的样本点满足以下条件：</p>
<script type="math/tex; mode=display">
w^Tx_i+b \geq \space\space1  \space\space\space\space y_i=+1\\
w^Tx_i+b \leq -1 \space\space\space\space y_i=-1\\</script><p>计算两个离超平面最近的正负样本的间隔距离，该样本点称为<strong>支持向量</strong>：</p>
<p>​        </p>
<p>​        在n维空间中，分割超平面方程：</p>
<script type="math/tex; mode=display">
f(x) = w_1x_1+w_2x_2+w_3x_3...w_nx_n+b \\</script><p>​        假设距离分割超平面最近距离的样本点是$x_k$，计算距离：</p>
<script type="math/tex; mode=display">
\space\space d = \frac{|w^{(1)}x_{k}^{(1)}+w^{(2)}x_{k}^{(2)}+w^{(3)}x_{k}^{(3)}....+w^{(n)}x_{k}^{(n)}+b|}{||\vec{w}||}=\frac{1}{||\vec{w}||}</script><script type="math/tex; mode=display">
间隔距离\space D =2*d = \frac{2}{||\vec{w}||}\\</script><p>因为$||\vec{w}||&gt;0$ 所以可以 $||\vec{w}||^2$代替，方便计算。</p>
<p>SVM的目的是在能够满足上述不等式的情况下，使得间隔距离最大，使得分类效果最好：</p>
<script type="math/tex; mode=display">
\max_{(w,b)}\space \frac{2}{||\vec{w}||^2}\\
s.t. \space y_i(w^Tx_i+b) \geq 1\space , i=1,2,3..n</script><p>转换成等价问题，得出目标函数：</p>
<script type="math/tex; mode=display">
\min_{(w,b)}\space \frac{1}{2}{||\vec{w}||^2}\\
\space\\
s.t. \space y_i(w^Tx_i+b) -1\geq 0</script><p>该问题带有不等式约束，可由拉格朗日乘子法求解。</p>
<h3 id="1-2-拉格朗日函数"><a href="#1-2-拉格朗日函数" class="headerlink" title="1.2     拉格朗日函数"></a>1.2     拉格朗日函数</h3><p>运用拉格朗日函数将不等式约束引入到目标函数中，方便求解：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) = \frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) ),\space\space \alpha_i\geq0</script><p>$L(w,b,\alpha) $对$\alpha$求极大值，就等价于原目标函数</p>
<script type="math/tex; mode=display">
\max_{\alpha}L(w,b,\alpha) = 
\left\{ \begin{array} { l } { 

 \frac{1}{2}{||\vec{w}||^2} ,(1-y_i(w^Tx_i+b) )<0

} \\ {

+\infin,\space\space\space\space\space (1-y_i(w^Tx_i+b))\geq0

} \end{array} \right.\\
\space\\
\space\space\space\space\space\space=\left\{ \begin{array} { l } { 

 \frac{1}{2}{||\vec{w}||^2} ,(y_i(w^Tx_i+b)-1 )\geq0 \space\space \space\space\space\space

} \\ {

+\infin,\space\space\space\space\space (y_i(w^Tx_i+b)-1)<0 \space\space \space\space\space\space

} \end{array} \right.\</script><hr>
<ol>
<li>如果 $(1-y_i(w^Tx_i+b) )&lt;0$ ，那么 $\alpha_i(1-y_i(w^Tx_i+b) )$ 一定小于等于0，为了使的$L(w,b,\alpha)$取得极大值，$\alpha_i$ 一定等于0，这种情况下$\max L(w,b,\alpha)= \frac{1}{2}{||\vec{w}||^2} $ ，且满足不等式约束。</li>
<li>如果 $(1-y_i(w^Tx_i+b) )\geq0$ ，那么$\alpha_i$可以取任意无限大的值使得$L(w,b,\alpha)$趋于无限大</li>
</ol>
<hr>
<p>再对上式求极小，可得：</p>
<script type="math/tex; mode=display">
\min_{w,b}\max_{\alpha}L(w,b,\alpha) = \min_{w,b}\left\{ \begin{array} { l } { 

 \frac{1}{2}{||\vec{w}||^2} ,(y_i(w^Tx_i+b)-1 )\geq0 \space\space \space\space\space\space

} \\ {

+\infin,\space\space\space\space\space (y_i(w^Tx_i+b)-1)<0 \space\space \space\space\space\space

} \end{array} \right.\\=\min_{w,b}\space \frac{1}{2}{||\vec{w}||^2},\space\space\space(y_i(w^Tx_i+b)-1 )\geq0 \space\space\space(原目标问题)</script><p>这样，就将带有不等式约束条件的优化问题，转换成了对函数$L(x,w,\alpha)$先求极大再求极小的问题，即目标问题转换成：</p>
<script type="math/tex; mode=display">
目标问题等价于\space ：\space \min_{w,b}\max_{\alpha}L(w,b,\alpha) = \min_{w,b}\max_{\alpha}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space),\space\space \alpha_i\geq0</script><h3 id="1-3-对偶问题和KKT条件"><a href="#1-3-对偶问题和KKT条件" class="headerlink" title="1.3     对偶问题和KKT条件"></a>1.3     对偶问题和KKT条件</h3><p>什么是对偶问题，通俗描述，就是将复杂问题转换成简单问题求解的方法</p>
<p>要想求解极值问题：</p>
<script type="math/tex; mode=display">
\min_{w,b}\max_{\alpha}L(w,b,\alpha) =\min_{w,b}\max_{\alpha}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space),\space\space \alpha_i\geq0\space\space\space\space\space---- -(1)</script><p>我们可以将问题转换成这样的形式求解，能够简化问题的求解过程(后续会说明SVM为什么使用对偶)：</p>
<script type="math/tex; mode=display">
\max_{\alpha}\min_{w,b}L(w,b,\alpha) =\max_{\alpha}\min_{w,b}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space),\space\space \alpha_i\geq0\space\space\space\space\space---- -(2)</script><p>假设(1)式的解为$w^{<em>},b^{</em>},\alpha^{<em>}$ ，式(2)的解为$w^{</em>d},b^{<em>d},\alpha^{</em>d}$ ，需证明</p>
<script type="math/tex; mode=display">
L(w^*,b^*,\alpha^*)= L(w^{*d},b^{*d},\alpha^{*d})</script><hr>
<p>拉格朗日对偶性定理：</p>
<p>​    带不等式约束和等式约束的优化问题</p>
<p>​    </p>
<script type="math/tex; mode=display">
\min_x f(x)\\
s.t.    c_i(x)\leq0, h_j(x)=0</script><p>​    其拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(x,\alpha,\beta) = f(x)+\sum_i^m\alpha_ic_i(x)+\sum_j^n\beta_jh_j(x)</script><p>​    假设，原问题  $\max_{\alpha,\beta}\min_xL(x,\alpha,\beta)$  最优解为$x^<em>,\alpha^</em>,\beta^<em>$，对应的最优值为$p^</em>$    </p>
<p>​            对偶问题  $\min_x\max_{\alpha,\beta}L(x,\alpha,\beta)$  最优解为$x^{<em>d},\alpha^{</em>d},\beta^{<em>d}$，对应的最优值为$d^</em>$    </p>
<p>​    </p>
<p>​       定理1：</p>
<script type="math/tex; mode=display">
d^*=L(x^*,\alpha^*,\beta^*)\leq L(x^{*d},\alpha^{*d},\beta^{*d})=p^*</script><p>​    定理2:</p>
<p>​        如果 1. $f(x),c_i(x)$ 为凸函数    2. $h_j(x)$ 为仿射函数(即最高次数为1的多项式函数)    3. $c_i(x)$ 严格可行</p>
<p>​        那么：</p>
<p>​    </p>
<script type="math/tex; mode=display">
1.    一定存在p^*=d^*\\2. \space p^*=d^*\space\space\space的充要条件\space\space\space<==> \space\space\space KKT条件：
\left\{ \begin{array} { l } { 
\frac{d}{dx}L(x^*,\alpha^*,\beta^*) = 0

}\\{ 
\alpha^{*}_ic^{*}_i=0  \space\space\space (松弛互补条件)



}\\{

c_i^{*}(x)\leq0


}\\{
    \alpha_i^*\geq0
}\\{
    h_j^*(x)=0
}
\end{array}\right.</script><hr>
<p>在原问题中，$ \frac{1}{2}{||\vec{w}||^2} ,(y_i(w^Tx_i+b)-1 )$  均为凸函数，根据上述定理2可知，只要满足KKT条件，就可以用求解对偶问题来代替求解原问题。</p>
<p>综上，原问题等价为：</p>
<script type="math/tex; mode=display">
\max_{\alpha}\min_{w,b}L(w,b,\alpha) =\max_{\alpha}\min_{w,b}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space\space)\\
解必须满足：
\left\{ \begin{array} { l } { 


\alpha_i(1-y_i(w^Tx_i+b) )=0



}\\{

(1-y_i(w^Tx_i+b) )\leq0

}\\{
    \alpha_i\geq0
}
\end{array}\right.</script><h3 id="1-4-求解过程"><a href="#1-4-求解过程" class="headerlink" title="1.4     求解过程"></a>1.4     求解过程</h3><p>目标函数：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha)=\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space\space</script><p>先求解：</p>
<script type="math/tex; mode=display">
\min_{w,b}L(w,b,\alpha)</script><p>分别对 $w,b$ 求导,并令其为0：</p>
<script type="math/tex; mode=display">
w-\sum_i^N\alpha_iy_ix_i =0\\
\sum_i^N \alpha_iy_i=0</script><p>将结果带入$L(w,b,\alpha)$ :</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) = \frac{1}{2}(\sum_i^N\alpha_iy_ix_i)^T\dot{}{(\sum_i^N\alpha_iy_ix_i)}-\sum_i^N\alpha_iy_i(\space(\sum_j^N\alpha_iy_ix_i)^T\dot{}{}x_i+b\space )+\sum_i^N\alpha_i\\
=-\frac{1}{2}\sum_i^N\sum_j^N\alpha_i\alpha_jy_iy_jx_i^Tx_j+\sum_i^N\alpha_i\\
s.t.\space\sum_i^N \alpha_iy_i=0,\alpha_i\geq0</script><p>现在参数只剩下$\alpha,b$，$\alpha$是一个N维的向量，要求：</p>
<script type="math/tex; mode=display">
\max_\alpha L(\alpha)\\
s.t.\space\sum_i^N \alpha_iy_i=0,\alpha_i\geq0</script><p>最后的参数用SMO求解，文章最后会介绍。</p>
<p>用SMO求解不但要用到这个最终表达式，还是用到对偶问题解成立的KKT条件：</p>
<script type="math/tex; mode=display">
\left\{ \begin{array} { l } { 


\alpha_i(1-y_i(w^Tx_i+b) )=0



}\\{

(y_i(w^Tx_i+b)-1 )\geq0

}\\{
    \alpha_i\geq0
}
\end{array}\right.</script><h2 id="2-非线性可分样本和核函数"><a href="#2-非线性可分样本和核函数" class="headerlink" title="2    非线性可分样本和核函数"></a>2    非线性可分样本和核函数</h2><p>接上，最后的结果：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) 
=-\frac{1}{2}\sum_i^N\sum_j^N\alpha_i\alpha_jy_iy_jx_i^Tx_j+\sum_i^N\alpha_i\\
s.t.\space\sum_i^N \alpha_iy_i=0,\alpha_i\geq0</script><p>该结果来源于最初的假设，就是样本线性可分，我们用一个简单地线性超平面来分割样本集：</p>
<script type="math/tex; mode=display">
f(x) = w_1x_1+w_2x_2+w_3x_3...w_nx_n+b \\</script><p><img src="/2018/10/13/SVM/kernal.png" alt="kernal"></p>
<p>假设二维的情况，用一条直线去分割二维空间,如右图：</p>
<script type="math/tex; mode=display">
f(x) =w_1x_1+w_2x_2+b</script><p>如果已知的样本点是非线性可分，如左图，那么最好的分割曲线应该是椭圆状曲线：</p>
<script type="math/tex; mode=display">
f(x) =w_1x_1^2+w_2x_2^2+b</script><p>设 $\phi(x)$ 为映射函数 </p>
<script type="math/tex; mode=display">
线性可分下：\phi_1(x) = x</script><script type="math/tex; mode=display">
椭圆情况下的映射：\phi_2(x) = x^2</script><p>定义函数：</p>
<script type="math/tex; mode=display">
K_1(x_1,x_2) = \phi_1(x_1)\dot{}{\phi_1(x_2)}={x_1}\dot{}{x_2}\\
\space\\
K_2(x_1,x_2)= \phi_2(x_1)\dot{}{\phi_2(x_2)}=x_1^2\dot{}{}x_2^2</script><p>根据上述推导，线性可分情况的解：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) 
=-\frac{1}{2}\sum_i^N\sum_j^N\alpha_i\alpha_jy_iy_jK_1(x_i,x_j)+\sum_i^N\alpha_i\\
s.t.\space\sum_i^N \alpha_iy_i=0,\alpha_i\geq0</script><p>同理可得出线性不可分，椭圆曲线分割线情况下的解：</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) 
=-\frac{1}{2}\sum_i^N\sum_j^N\alpha_i\alpha_jy_iy_jK_2(x_i,x_j)+\sum_i^N\alpha_i\\
s.t.\space\sum_i^N \alpha_iy_i=0,\alpha_i\geq0</script><p>形如这种，能够将样本点映射到特定空间    的函数  $K(x_i,x_j)$ 称为核函数</p>
<p>常用核函数</p>
<script type="math/tex; mode=display">
线性核：\space \space K \left( \boldsymbol { x } _ { i } , \boldsymbol { x } _ { j } \right) = \boldsymbol { x } _ { i } ^ { \mathrm { T } } \boldsymbol { x } _ { j }\\

多项式核：\space\space
K\left( \boldsymbol { x } _ { i } , \boldsymbol { x } _ { j } \right) = \left(\xi+\gamma \boldsymbol { x } _ { i } ^ { \mathrm { T } } \boldsymbol { x } _ { j } \right) ^ { d }\\

\left. \begin{array} { l } {高斯核： K \left( \boldsymbol { x } _ { i } , \boldsymbol { x } _ { j } \right) =exp \left( - \gamma { \left\| \boldsymbol { x } _ { i } - \boldsymbol { x } _ { j } \right\| ^ { 2 } } \right) }\\ { 拉普拉斯核：K \left( \boldsymbol { x } _ { i } , \boldsymbol { x } _ { j } \right) = \exp \left( - \frac { \left\| \boldsymbol { x } _ { i } - \boldsymbol { x } _ { j } \right\| } { \sigma } \right) } \\ {sigmoid核： K \left( \boldsymbol { x } _ { i } , \boldsymbol { x } _ { j } \right) = \tanh \left( \gamma \boldsymbol { x } _ { i } ^ { \mathrm { T } } \boldsymbol { x } _ { j } + \xi \right) } \end{array} \right.</script><h2 id="3-软间隔SVM"><a href="#3-软间隔SVM" class="headerlink" title="3     软间隔SVM"></a>3     软间隔SVM</h2><p>有些数据集虽然大体上是线性可分的，但会有一些点，可能是噪点或是异常点，使得求出严格的分割平面较为困难，软间隔就是为了解决这样的问题，通过放松条件，允许每个点都可以有<u><strong>一定程度的越界</strong></u>，引入参数 $\xi_i$ 来表示每个样本点的越界程度，该参数称为<u><strong>松弛变量</strong></u>，最后通过最小化该参数使所有点的越界程度最小，即达到最好的分类效果，也容忍了一定程度的噪点问题 。 </p>
<p><img src="/2018/10/13/SVM/softsvm-8314268.png" alt="softsvm-8314268"></p>
<p>分类条件：</p>
<script type="math/tex; mode=display">
w^Tx_i+b \geq \space\space1-\xi_i  \space\space\space\space y_i=+1\\
w^Tx_i+b \geq \xi_i-1 \space\space\space\space\space y_i=-1\\</script><p>优化目标：</p>
<script type="math/tex; mode=display">
\min _ { w , b , \xi } \frac { 1 } { 2 } \| w \| ^ { 2 } + C \sum _ { i = 1 } ^ { N } \xi _ { i }</script><script type="math/tex; mode=display">
\text { s.t. } \quad y _ { i } \left( w \cdot x _ { i } + b \right) \geqslant 1 - \xi _ { i } , \space\space\space
\xi _ { i } \geqslant 0</script><p>C称为惩罚因子，C值得大小表征对越界的容忍程度，即惩罚力度的大小。对松弛参数的理解，可以<u>类比在其他学习算法中的正则项，通过引入正则项来防止过拟合</u>。如上左图，如果用硬间隔SVM，中间那个异常点将会起到支持向量的作用，那么正负样本的间隔将会非常小，这可以类比成一种过拟合。</p>
<p>推导过程与不带松弛参数的线性SVM类似：</p>
<p>写出拉格朗日方程：</p>
<script type="math/tex; mode=display">
L ( w , b , \xi , \alpha , \mu ) \equiv \frac { 1 } { 2 } \| w \| ^ { 2 } + C \sum _ { i = 1 } ^ { N } \xi _ { i } - \sum _ { i = 1 } ^ { N } \alpha _ { i } \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right) - \sum _ { i = 1 } ^ { N } \mu _ { i } \xi _ { i }\\
s.t.\alpha_i\geq0,\mu_i\geq0</script><p>原问题：</p>
<script type="math/tex; mode=display">
\min_{w,b,\xi}\max_{\alpha,\mu}L ( w , b , \xi , \alpha , \mu )</script><p>转换成对偶问题：</p>
<script type="math/tex; mode=display">
\max_{\alpha,\mu}\min_{w,b,\xi}L ( w , b , \xi , \alpha , \mu )=\min_{w,b,\xi}\max_{\alpha,\mu}L ( w , b , \xi , \alpha , \mu )\\
s.t. \space\space KKT</script><p>转换成对偶问题后就可以先对$w,b,\xi$ 求导求极小值：</p>
<script type="math/tex; mode=display">
\left.\begin{aligned} \nabla _ { w } L ( w , b , \xi , \alpha , \mu ) & = w - \sum _ { i = 1 } ^ { N } \alpha _ { i } y _ { i } x _ { i } = 0 \\ \nabla _ { b } L ( w , b , \xi , \alpha , \mu ) & = - \sum _ { i = 1 } ^ { N } \alpha _ { i } y _ { i } = 0 \\ \nabla _ { \xi _ { i } } L ( w , b , \xi , \alpha , \mu ) & = C - \alpha _ { i } - \mu _ { i } = 0 \end{aligned} \right.</script><p>回带到原函数，可以发现，正好消去了$\xi,w,b$</p>
<p>最后，又得出以下熟悉的形式</p>
<script type="math/tex; mode=display">
\min _ { \alpha } L ( w , b , \xi , \alpha , \mu ) = - \frac { 1 } { 2 } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } \left( x _ { i } ^T x _ { j } \right) + \sum _ { i = 1 } ^ { N } \alpha _ { i }\\\left. \begin{array} { c l } { \text { s.t. } } & { \sum _ { i = 1 } ^ { N } \alpha _ { i } y _ { i } = 0 } \\ { } & { 0 \leqslant \alpha _ { i } \leqslant C },{C=\alpha_i+\mu_i} \end{array} \right.</script><p>与之前的推导结果唯一不同的是 $\alpha_i$ 有了上界的限制， 该限制来自于 上述对 $\xi$ 求导的结果 $C - \alpha _ { i } - \mu _ { i } = 0$</p>
<p>最后，相应的KKT条件：</p>
<script type="math/tex; mode=display">
\left\{\begin{array} { l } {  \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)\geqslant0} \\ {\alpha_i \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)=0  } \\ { \alpha _ { i } \geqslant 0 } \\ { \mu _ { i } \geqslant 0  }\\{\xi_i\geqslant0}\\{\mu_i\xi_i=0} \end{array} \right.</script><h2 id="4-SMO算法"><a href="#4-SMO算法" class="headerlink" title="4     SMO算法"></a>4     SMO算法</h2><p>SMO算法SVM的最后一步，是用于计算SVM的最后参数$\alpha$的一种优化算法。</p>
<p>以最一般化的SVM来做SMO的计算，带软间隔，带核函数的情况：</p>
<script type="math/tex; mode=display">
分割面:f(x_i) = w\phi (x_i)+b = \sum_j^N\alpha_jy_jK(x_i,x_j)+b\space\space\space\space\space---(1)\\\space\space\space\space\space(w=\sum_j^N\alpha_jy_j\phi(x_j))</script><script type="math/tex; mode=display">
优化目标：\min _ { \alpha } L ( w , b , \xi , \alpha , \mu ) = - \frac { 1 } { 2 } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { N } \alpha _ { i } \alpha _ { j } y _ { i } y _ { j } K\left( x _ { i } , x _ { j } \right) + \sum _ { i = 1 } ^ { N } \alpha _ { i }\\\left. \begin{array} { c l } { \text { s.t. } } & { \sum _ { i = 1 } ^ { N } \alpha _ { i } y _ { i } = 0 } \\ { } & { 0 \leqslant \alpha _ { i } \leqslant C } ,{C=\alpha_i+\mu_i}\end{array} \right.\\</script><script type="math/tex; mode=display">
KKT条件：\left\{\begin{array} { l } {  \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)\geqslant0} \\ {\alpha_i \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)=0  } \\ { \alpha _ { i } \geqslant 0 } \\ { \mu _ { i } \geqslant 0  }\\{\xi_i\geqslant0}\\{\mu_i\xi_i=0} \end{array} \right.</script><p>计算步骤：</p>
<p>​    每次迭代只选择N个$\alpha_i$中的两个$\alpha_i$ 做优化，最终达到所有$\alpha$收敛的效果</p>
<p>​    指定一个分类精度$\varepsilon$，来控制收敛的精度</p>
<p>​    1.     随机初始化所有$\alpha$ ，$b$  </p>
<p>​    2.    根据$f(x_i)$ 计算误差 </p>
<script type="math/tex; mode=display">
E_i= f(x_i)-y_i\space\space\space\space---(2)</script><p>​        3.     循环选择一个最不满足KKT条件的 $\alpha_v$ ，如下选择规则 ：</p>
<script type="math/tex; mode=display">
\alpha_v<C \space\space\space and \space\space\space y_i(x_i)E_v <-\varepsilon \\or\\
0<\alpha_v \space\space\space and \space\space\space  y_i(x_i)E_v >\varepsilon\\</script><script type="math/tex; mode=display">
\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space---(3)</script><p>​        如果找不到，退出循环，优化结束</p>
<p>​        有些算法用随机选择的方法选择第一$\alpha_v$ 这样的做法收敛较慢</p>
<blockquote>
<p>面试题：什么是最不满足KKT条件的$\alpha_i$</p>
<p>我们这里说的条件 主要是松弛互补条件，即</p>
<p>$\alpha_i \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)=0$</p>
<p>转换形式：</p>
<p>$\alpha_i \left( y _ { i }( f(x_i)-y_i(x))+ \xi _ { i } \right)=0\\=&gt;\alpha_i(y_i(x_i)E_i+\xi_i)=0$</p>
<p>我们考虑最重要的几个$\alpha_i$，即支持向量样本点对于的$\alpha_i$</p>
<p>因为${ 0 \leqslant \alpha _ { i } \leqslant C } $,$C=\alpha_i+\mu_i$,$\mu_i\xi_i=0$,$\alpha_i \left( y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i } \right)=0$</p>
<p>当$\alpha_i=0$ 时，对$y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i }$ 没有约束，即不是特殊的分类点</p>
<p>当$\alpha_i=C$时，$y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i }$ 一定等于0，则样本落在软间隔内部</p>
<p>当$0&lt;\alpha_i&lt;C$时，$y _ { i } \left( w \cdot x _ { i } + b \right) - 1 + \xi _ { i }$ 一定等于0，且$\xi_i$一定为0，该点为支持向量</p>
<p>最不满足KKT条件的点优先选择 <u>那些不满足KKT条件的支持向量</u></p>
<p>$0&lt;\alpha_i&lt;C$的情况下，$\xi_i$一定为0，那么只要$y_i(x_i)E_i$不等于0，那么$\alpha_i$就是违法KKT条件</p>
</blockquote>
<p>​    4.     根据选择出来的$\alpha_v$ 循环寻找$\alpha_w$ 使得 $|E_v-E_w|$ 最大</p>
<p>​    5.    计算L,H</p>
<script type="math/tex; mode=display">
if\space\space y_v==y_w\\
\space\\
L = \max \left( 0 , \alpha _ { w}  + \alpha _ { v } - C \right) , \quad H = \min \left( C , \alpha _ { w } + \alpha _ { v } \right)\\
\space\\
else\\
\space\\
L = \max \left( 0 , \alpha _ { w}  - \alpha _ { v }  \right) , \quad H = \min \left( C , C + \alpha _ { w }- \alpha _ { v } \right)\\</script><p>​    6.      更新$\alpha_w$ 的值</p>
<p>​    </p>
<script type="math/tex; mode=display">
\alpha _ { w} ^ {’ } = \alpha _ { w } ^ { \mathrm { old } } + \frac { y _ {w} \left( E _ { v } - E _ { w } \right) } { 
\eta
}\\
\space\\
\eta=K(x_v,x_v)+K(x_w,x_w)-2K(x_v,x_w)\space\space\space\space(\eta\geq0)\space\space\space---(4)</script><script type="math/tex; mode=display">
\alpha _ { w} ^ { \mathrm { new } } = \left\{ \begin{array} { l l } { H , } & { \alpha _ { w} ^ {'} > H } \\ { \alpha _ { w } ^ { '} } & { L \leqslant \alpha _ { w } ^ { '} \leqslant H } \\ { L , } & { \alpha _{ w } ^ { '} < L } \end{array} \right.\space\space\space\space---(5)</script><p>​    7.     更新$\alpha_v$的值</p>
<script type="math/tex; mode=display">
\alpha _ { v } ^ { \mathrm { new } } = \alpha _ { v} ^ { \mathrm { old } } + y _ { 1 } y _ { 2 } \left( \alpha _ { w } ^ { \mathrm { old } } - \alpha _ { w } ^ { \mathrm { new } } \right)\space\space\space---(6)</script><p>​    8.    计算$b$</p>
<p>​    </p>
<script type="math/tex; mode=display">
{ b } _ { v } ^ { \mathrm { new } } = - E _ {v } - y _ { v } K (x_v,x_v) \left( \alpha _ { v } ^ { \mathrm { new } } - \alpha _ { v } ^ { \mathrm { old } } \right) - y _ { w } K (x_w,x_v) \left( \alpha _ { w } ^ { \mathrm { new } } - \alpha _ { w } ^ { \mathrm { old } } \right) + b ^ { \mathrm { old } }</script><script type="math/tex; mode=display">
{ b } _ { w } ^ { \mathrm { new } } = - E _ {w } - y _ { v } K (x_v,x_w) \left( \alpha _ { v } ^ { \mathrm { new } } - \alpha _ { v } ^ { \mathrm { old } } \right) - y _ { w } K (x_w,x_w) \left( \alpha _ { w } ^ { \mathrm { new } } - \alpha _ { w } ^ { \mathrm { old } } \right) + b ^ { \mathrm { old } }</script><script type="math/tex; mode=display">
if\space\space\space 0<\alpha_v^{new}<C:\\
\space\\
b^{new} =b_v^{new}\\
\space\\
else\space\space if\space  \space\space0<\alpha_w^{new}<C:\\

\space\\
b^{new} =b_w^{new}\\
\space\\
else:\\
b^{new} = \frac{1}{2}(b_v^{new}+b_w^{new})</script><p>​    9.    跳回2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SVM</span><span class="params">(X, y, C, e, maxEpoch)</span>:</span> </span><br><span class="line"></span><br><span class="line">    <span class="comment">#N是样本数，D是样本维度</span></span><br><span class="line">    N,D = shape(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化alpha,b</span></span><br><span class="line">    alphas = np.zeros(N,<span class="number">1</span>)</span><br><span class="line">    b = <span class="number">0</span>; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(maxEpoch):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> range(N):  <span class="comment">#在数据集上遍历每一个alpha</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#(1)式</span></span><br><span class="line">            <span class="comment">#如果需要加核函数 ： fx_v = (alphas*y).T*K(x,x[v]) + b </span></span><br><span class="line">            fx_v = (alphas*y).T*(X.dot(X[v].T)) + b </span><br><span class="line"></span><br><span class="line">            <span class="comment">#(2)式</span></span><br><span class="line">            E_v  = fx_v-y[v] </span><br><span class="line">            </span><br><span class="line">            <span class="comment">#根据(3)式规则选择alpha_v</span></span><br><span class="line">            <span class="keyword">if</span> ( ( y[v]*E_v &lt; -e ) <span class="keyword">and</span> ( alphas[v]&lt;C ) ) <span class="keyword">or</span> \</span><br><span class="line">               ( ( y[v]*E_v &gt;  e ) <span class="keyword">and</span> ( alphas[v]&gt;<span class="number">0</span> ) ): </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据第四点选择alpha_w</span></span><br><span class="line">                w = <span class="number">-1</span></span><br><span class="line">                E_w = <span class="number">-1</span></span><br><span class="line">                tempMax = <span class="number">-1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</span><br><span class="line">                    <span class="keyword">if</span> j==v:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    fx_j = (alphas*y).T*(X.dot(X[j].T)) + b </span><br><span class="line">                    E_j  = fx_j-y[j]</span><br><span class="line">                    <span class="keyword">if</span> abs(E_v-E_j) &gt; tempMax:</span><br><span class="line">                        E_w = E_j</span><br><span class="line">                        w = j</span><br><span class="line">              </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据第五点计算L和H</span></span><br><span class="line">                <span class="keyword">if</span>(y[v]!=y[w]):  </span><br><span class="line">                    L=max(<span class="number">0</span>, alphas[w]-alphas[v]) </span><br><span class="line">                    H=min(C, C+alphas[w]-alphas[v]) </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    L=max(<span class="number">0</span>, alphas[w]+alphas[v]-C) </span><br><span class="line">                    H=min(C, alphas[w]+alphas[v]) </span><br><span class="line">                <span class="keyword">if</span> L==H: </span><br><span class="line">                    print(<span class="string">'L==H'</span>) </span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据式4计算eta</span></span><br><span class="line">                eta= X[i]*X[i].T + X[j]*X[j].T - <span class="number">2.0</span>*X[v]*X[w].T</span><br><span class="line">                <span class="keyword">if</span> eta&lt;<span class="number">0</span>: </span><br><span class="line">                    print(<span class="string">'eta&lt;0'</span>) </span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据式5计算更新alpha_w</span></span><br><span class="line">                alpha_w_old = alphas[w]</span><br><span class="line">                alphas[w]+=y[w]*(E_v-E_w)/eta  <span class="comment">#调整alphas[j] </span></span><br><span class="line">                <span class="keyword">if</span> alphas[w]&gt;H: </span><br><span class="line">                    alphas[w]=H </span><br><span class="line">                <span class="keyword">if</span> alphas[w]&lt;L: </span><br><span class="line">                    alphas[w]=L </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#如果变化很小，后面就不做了，直接下一轮更新</span></span><br><span class="line">                <span class="keyword">if</span>(abs(alphas[w]-alpha_w_old)&lt;<span class="number">0.00001</span>):  </span><br><span class="line">                    print(<span class="string">'w not moving enough'</span>)</span><br><span class="line">                    <span class="keyword">continue</span> </span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据式6计算更新alpha_v</span></span><br><span class="line">                alpha_v_old=alphas[v]</span><br><span class="line">                alphas[v]+=y[w]*y[v]*(alpha_w_old-alphas[w])  <span class="comment">#调整alphas[i]</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment">#根据第八点计算b</span></span><br><span class="line">                b_v = b-E_v-\</span><br><span class="line">                y[v]*(alphas[v]-alpha_v_old)*X[v]*X[v].T-\</span><br><span class="line">                y[w]*(alphas[w]-alpha_w_old)*X[w]*X[v].T </span><br><span class="line">                </span><br><span class="line">                b_w = b-E_w-\</span><br><span class="line">                y[v]*(alphas[v]-alpha_v_old)*X[v]*X[w].T-\</span><br><span class="line">                y[w]*(alphas[w]-alpha_w_old)*X[w]*X[w].T </span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span>(<span class="number">0</span>&lt;alphas[i]) <span class="keyword">and</span> (C&gt;alphas[i]): </span><br><span class="line">                    b=b_v </span><br><span class="line">                <span class="keyword">elif</span>(<span class="number">0</span>&lt;alphas[j]) <span class="keyword">and</span> (C&gt;alphas[j]): </span><br><span class="line">                    b=b_w </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    b=(b_v+b_w)/<span class="number">2.0</span> </span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> b, alphas</span><br></pre></td></tr></table></figure>
<h2 id="5-SVM面试常考题"><a href="#5-SVM面试常考题" class="headerlink" title="5     SVM面试常考题"></a>5     SVM面试常考题</h2><h3 id="5-1-为什么要转化成对偶问题求解SVM"><a href="#5-1-为什么要转化成对偶问题求解SVM" class="headerlink" title="5.1     为什么要转化成对偶问题求解SVM"></a>5.1     为什么要转化成对偶问题求解SVM</h3><p>​    原问题：</p>
<script type="math/tex; mode=display">
\min_{w,b}\max_{\alpha}L(w,b,\alpha) =\min_{w,b}\max_{\alpha}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space),\space\space \alpha_i\geq0\space\space\space\space\space---- -(1)</script><p>​    对偶问题：</p>
<script type="math/tex; mode=display">
\max_{\alpha}\min_{w,b}L(w,b,\alpha) =\max_{\alpha}\min_{w,b}（\space\space\space\frac{1}{2}{||\vec{w}||^2} +\sum_i^N\alpha_i(1-y_i(w^Tx_i+b) )\space\space\space),\space\space \alpha_i\geq0\space\space\space\space\space---- -(2)</script><ul>
<li>求解问题的复杂度不同，原问题最外层的是$\min_{w,b}$ ， 最后求解问题的复杂度和$w$的维度相关，即，<u>和样本的维度相关</u>。而对偶问题的外层是$max_{\alpha}$ ，最后的求解问题与$\alpha$的维度相关，即，<u>和样本数量相关</u>。传统机器学习最初是针对高纬度低样本数的数据进行设计的(人工智能课老师)，所以svm对于高维空间中较稀疏的样本表现较好。</li>
<li>能够运用核函数 (主要原因)，$L(w,b,\alpha) $只有先对$w,b$求导才能导出最后带有 $(x_i\dot{}{x_j})$ 的形式，才有机会运用核函数来应对非线性情况</li>
<li>引入了KKT条件，简化了约束条件</li>
</ul>
<h3 id="5-2-核函数的选择问题"><a href="#5-2-核函数的选择问题" class="headerlink" title="5.2     核函数的选择问题"></a>5.2     核函数的选择问题</h3><p>​    吴恩达的选择：</p>
<ul>
<li>特征数量很大，     跟样本差不多时，  用线性核</li>
<li>特征数量比较小， 样本数量很大，      需手动添加特征，后用线性核</li>
<li>特征数量比较小， 样本数量一般，      用高斯核</li>
</ul>
<p>​    从调参复杂度选择</p>
<p>​    线性核：      无</p>
<p>​    多项式核：  幂次d, 系数$\gamma$,  常数项$\xi$</p>
<p>​    高斯核：         系数$\gamma$</p>
<p>​    sigmoid核：系数$\gamma$,常数项$\xi$</p>
<h3 id="5-3-sklearn中SVM的参数使用"><a href="#5-3-sklearn中SVM的参数使用" class="headerlink" title="5.3     sklearn中SVM的参数使用"></a>5.3     sklearn中SVM的参数使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">svm</span>.<span class="title">SVC</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">            C=<span class="number">1.0</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            kernel=<span class="string">'rbf'</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            degree=<span class="number">3</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            gamma=<span class="string">'auto'</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            coef0=<span class="number">0.0</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            shrinking=True, </span></span></span><br><span class="line"><span class="class"><span class="params">            probability=False, </span></span></span><br><span class="line"><span class="class"><span class="params">            tol=<span class="number">0.001</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            cache_size=<span class="number">200</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            class_weight=None, </span></span></span><br><span class="line"><span class="class"><span class="params">            verbose=False, </span></span></span><br><span class="line"><span class="class"><span class="params">            max_iter=<span class="number">-1</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            decision_function_shape=<span class="string">'ovr'</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">            random_state=None)</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">主要需要调整的参数：</span></span><br><span class="line"><span class="class"><span class="title">C</span>:</span>	对应上文的C，软间隔中的惩罚因子</span><br><span class="line">kernel:	核函数 ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ，默认rbf</span><br><span class="line">degree: kernel为多项式核的时候有效，调整的是多项式核的幂次</span><br><span class="line">gamma:  当kernel为‘rbf’, ‘poly’或‘sigmoid’时生效，对应的核函数中的gama值</span><br><span class="line">coef0:  当kernel为‘poly’,‘sigmoid’时有效</span><br><span class="line">tol  :  训练精度，对应上面SMO的精度</span><br><span class="line">max_iter: 对应上述的maxEpoch</span><br></pre></td></tr></table></figure>
<h3 id="5-4-样本类别不均衡情况是否影响SVM分类效果"><a href="#5-4-样本类别不均衡情况是否影响SVM分类效果" class="headerlink" title="5.4     样本类别不均衡情况是否影响SVM分类效果"></a>5.4     样本类别不均衡情况是否影响SVM分类效果</h3><p>会有影响，原因：</p>
<script type="math/tex; mode=display">
\min _ { w , b , \xi } \frac { 1 } { 2 } \| w \| ^ { 2 } + C \sum _ { i = 1 } ^ { N } \xi _ { i }</script><p>假设负样本的数量远大于正样本的数量，那么负样本越界的点一定大于正样本越界的点，算法最小化 $C\sum_i^{N}\xi_i$  会使得超平面往正样本移动，来减少负样本越界的点，从而减小   $C\sum_i^{N}\xi_i$。</p>
<p>如何解决：</p>
<ol>
<li>对正样本和负样本设置不同的C，来保证平衡</li>
<li>欠采样</li>
</ol>
<h3 id="5-5-SVM多分类问题"><a href="#5-5-SVM多分类问题" class="headerlink" title="5.5     SVM多分类问题"></a>5.5     SVM多分类问题</h3><p>一对一法：</p>
<p>​    任意两类样本之间设计一个SVM，最终有k(k-1)/2个分类器，投票决定</p>
<p>一对多法：</p>
<pre><code> 把某个类别的样本归为一类，其余类别归为另一类，做一个SVM，这样，k个类别就有k个SVM，样本经过k个SVM分别计算，以函数值最大的那个类别为最终类别
</code></pre>
      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Chen Zhou
  </li>

  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/11/cs231n-dropout/" rel="next" title="cs231n Dropout">
                <i class="fa fa-chevron-left"></i> cs231n Dropout
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Chen Zhou" />
            
              <p class="site-author-name" itemprop="name">Chen Zhou</p>
              <p class="site-description motion-element" itemprop="description">Get busy living, or get busy dying.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/chenzhou9513" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="chenzhou9513@zju.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM-支持向量机"><span class="nav-number">1.</span> <span class="nav-text">SVM 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-线性svm"><span class="nav-number">1.2.</span> <span class="nav-text">1    线性svm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-目标函数"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.1     目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-拉格朗日函数"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2     拉格朗日函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-对偶问题和KKT条件"><span class="nav-number">1.2.3.</span> <span class="nav-text">1.3     对偶问题和KKT条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-求解过程"><span class="nav-number">1.2.4.</span> <span class="nav-text">1.4     求解过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-非线性可分样本和核函数"><span class="nav-number">1.3.</span> <span class="nav-text">2    非线性可分样本和核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-软间隔SVM"><span class="nav-number">1.4.</span> <span class="nav-text">3     软间隔SVM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-SMO算法"><span class="nav-number">1.5.</span> <span class="nav-text">4     SMO算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-SVM面试常考题"><span class="nav-number">1.6.</span> <span class="nav-text">5     SVM面试常考题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-为什么要转化成对偶问题求解SVM"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.1     为什么要转化成对偶问题求解SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-核函数的选择问题"><span class="nav-number">1.6.2.</span> <span class="nav-text">5.2     核函数的选择问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-sklearn中SVM的参数使用"><span class="nav-number">1.6.3.</span> <span class="nav-text">5.3     sklearn中SVM的参数使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-样本类别不均衡情况是否影响SVM分类效果"><span class="nav-number">1.6.4.</span> <span class="nav-text">5.4     样本类别不均衡情况是否影响SVM分类效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-SVM多分类问题"><span class="nav-number">1.6.5.</span> <span class="nav-text">5.5     SVM多分类问题</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Zhou</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  







  
  





  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_sphere.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
