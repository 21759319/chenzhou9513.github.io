<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Chenzhou" />










<meta name="description" content="Softmax 相关推导 (cs 231n版本)简介cs231n 版本的softmax 损失函数以及 梯度 推导 原note 地址 :  http://cs231n.github.io/linear-classify/">
<meta property="og:type" content="article">
<meta property="og:title" content="cs231n softmax 推导">
<meta property="og:url" content="http://yoursite.com/2018/10/07/cs231n-softmax/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Softmax 相关推导 (cs 231n版本)简介cs231n 版本的softmax 损失函数以及 梯度 推导 原note 地址 :  http://cs231n.github.io/linear-classify/">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-10-07T15:26:30.852Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cs231n softmax 推导">
<meta name="twitter:description" content="Softmax 相关推导 (cs 231n版本)简介cs231n 版本的softmax 损失函数以及 梯度 推导 原note 地址 :  http://cs231n.github.io/linear-classify/">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeIn","post_body":"fadeIn","coll_header":"fadeIn","sidebar":false}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/07/cs231n-softmax/"/>





  <title>cs231n softmax 推导 | Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/07/cs231n-softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Chen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">cs231n softmax 推导</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-07T19:24:24+08:00">
                2018-10-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/cs231n/" itemprop="url" rel="index">
                    <span itemprop="name">cs231n</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Softmax-相关推导-cs-231n版本"><a href="#Softmax-相关推导-cs-231n版本" class="headerlink" title="Softmax 相关推导 (cs 231n版本)"></a>Softmax 相关推导 (cs 231n版本)</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>cs231n 版本的softmax 损失函数以及 梯度 推导</p>
<p>原note 地址 :</p>
<p> <a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">http://cs231n.github.io/linear-classify/</a></p>
<a id="more"></a>
<h2 id="Softmax-损失函数"><a href="#Softmax-损失函数" class="headerlink" title="Softmax 损失函数"></a>Softmax 损失函数</h2><p>变量维度：        </p>
<p>样本总数 N, 特征维度m, 类别 n </p>
<p>w    -&gt;     [m,n] </p>
<p>x     -&gt;     [N,m]</p>
<p>y     -&gt;     [N,1]</p>
<script type="math/tex; mode=display">
\left.\begin{aligned}L ( w ) &= \prod _ { N } P \left( y _ { i } , x _ { i } | w \right)\\&=\sum _ { i } ^ { N } \log \left( \frac { e ^ { f _ { y _ { i } } } } { \sum _ { k = 1 } ^ { n } e ^ { f _ { k }  } } \right)\\&=\sum _ { i } ^ { N } \left( f _ { y _ { i } }  - \log \sum _ { k = 1 } ^ { n } e ^ { f _ { k }  } \right) \end{aligned} \right.</script><script type="math/tex; mode=display">
f(\vec{x_i};\vec{w}) = \vec{x_i}\cdot\vec{w}
f_j(\vec{x_i};\vec{w}) = (\vec{x_i}\cdot\vec{w})_j
softmax(f(\vec{x_i};\vec{w}) ) _j= \frac{e^{f_j}}{\sum_{k=1}^{n}{e^{f_k}}}</script><p>由softmax计算出来的结果 pred : [n,1]  和 样本真实值 y : [n,1] 计算损失函数：</p>
<p>先计算似然函数：</p>
<script type="math/tex; mode=display">
   L(w) = \prod_{N}{p(y_i,x_i|w)} \space\space\space\space\space  (即给定样本x_i,y_i的发生的概率)</script><script type="math/tex; mode=display">
   p(y_i,x_i|w) = softmax(f(\vec{x_i};\vec{w}) )_{y_i}\space\space\space\space\space</script><script type="math/tex; mode=display">
   如三分类 softmax(f(\vec{x_i};\vec{w}) ) = [0.1,0.5,0.4]\space \space ,y_i = 2,p(y_i,x_i|w) = 0.4</script><p>取对数：</p>
<script type="math/tex; mode=display">
L(w) = \sum_{i}^{N}{log(p(y_i,x_i|w))}\\
=\sum_{i}^{N}log( \frac{e^{f_{y_i}(x_i;w)} }{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}})\\
=\sum_{i}^{N} {( {f_{y_i}(x_i;w)}-log{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}})}</script><p>似然函数取极大，即定义对所有样本总损失函数为</p>
<script type="math/tex; mode=display">
loss(w) = -L(w) =\sum_{i}^{N}{(- {f_{y_i}(x_i;w)}+log{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}})}</script><p>取平均，再加入正则项</p>
<script type="math/tex; mode=display">
loss(w)  =\frac{1}{N}\sum_{i}^{N}{(- {f_{y_i}(x_i;w)}+log{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}})} \space\space + {\lambda}||w||^2</script><script type="math/tex; mode=display">
{\lambda}||w||^2 = \lambda\sum_{n}\sum_{m}(w_{n}^m)^2</script><h2 id="Softmax损失函数求导"><a href="#Softmax损失函数求导" class="headerlink" title="Softmax损失函数求导"></a>Softmax损失函数求导</h2><script type="math/tex; mode=display">
loss(w) =\frac{1}{N} \sum_{i}^{N} {( -{f_{y_i}}+log{\sum_{k=1}^{n}{e^{f_k}}})} \space\space + {\lambda}||w||^2</script><script type="math/tex; mode=display">
\frac{dL(\vec{w})}{dw_{u}} = \frac{1}{N}\sum_{i}^{N}(   \frac{e^{f_u(x_i;w)}x_i}{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}} -1(u==y_i)\space           )x_i \space\space + 2{\lambda}w_{u}</script><p>补充：</p>
<script type="math/tex; mode=display">
-\frac{d {f_{y_i}}}{dw_{u}}= -\frac{d}{dw_u}({x_i}\cdot{w_{y_i}})\\
if\space y_i \space ==\space u\space\space\space\space\space\space\space\space\space\space\space\space\space -\frac{d {f_{y_i}}}{dw_{u}}= -x_{i}\\</script><script type="math/tex; mode=display">
else \space\space\space\space\space\space\space\space\space\space\space\space\space -\frac{d {f_{y_i}}}{dw_{u}}=0</script><h2 id="Softmax矩阵形式求导"><a href="#Softmax矩阵形式求导" class="headerlink" title="Softmax矩阵形式求导"></a>Softmax矩阵形式求导</h2><p>向量形式求导方便计算，假设不考虑正则项，有：</p>
<script type="math/tex; mode=display">
\frac{dL(\vec{w})}{dw_{u}} = \frac{1}{N}\sum_{i}^{N}(\space ( \frac{e^{f_u(x_i;w)}}{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}}-1(u==y_i)\space                ) \space \space x_i\space) \space\space</script><p>设概率矩阵 $P$ ,维度$(N,n)$ ,定义如下：</p>
<script type="math/tex; mode=display">
P[i][j] = \frac{e^{f_j(x_i;w)}}{\sum_{k=1}^{n}{e^{f_k(x_i;w)}}}</script><p>设矩阵 MASK，维度$(N,n)$ ,定义如下：</p>
<script type="math/tex; mode=display">
M A S K [ i ] [ j ] = \left\{ \begin{array} { l } { 0 , j ! = y [ i ] } \\ { 1 , j = y [ i ] } \end{array} \right.</script><p>求导结果向量形式：</p>
<script type="math/tex; mode=display">
\frac{dL(\vec{w})}{d\vec{w}} =\frac{1}{N}x^T\dot{}{(P-MASK)}</script><p>加正则：</p>
<script type="math/tex; mode=display">
\frac{dL(\vec{w})}{d\vec{w}} =\frac{1}{N}x^T\dot{}{(P-MASK)}+ 2{\lambda}w</script><h2 id="实现代码-CS-231n-Assignment-1"><a href="#实现代码-CS-231n-Assignment-1" class="headerlink" title="实现代码    CS 231n    Assignment 1"></a>实现代码    CS 231n    Assignment 1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">  of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">  - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns a tuple of:</span></span><br><span class="line"><span class="string">  - loss as single float</span></span><br><span class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using explicit loops.     #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  fxw = np.dot(X,W)    </span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">    fxw_i = fxw[i]-max(fxw[i])</span><br><span class="line">    loss += -fxw_i[y[i]]+np.log(np.sum(np.exp(fxw_i)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(dW.shape[<span class="number">1</span>]):</span><br><span class="line">      <span class="keyword">if</span> j==y[i]:  </span><br><span class="line">        dW[:,j]+= -X[i]+(X[i]/np.log(np.sum(np.exp(fxw_i))))</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        dW[:,j]+=X[i]/np.log(np.sum(np.exp(fxw_i)))</span><br><span class="line">  </span><br><span class="line">  loss /= X.shape[<span class="number">0</span>] </span><br><span class="line">  loss +=  reg * np.sum(W * W)</span><br><span class="line">  dW = dW/X.shape[<span class="number">0</span>]  + <span class="number">2</span>*reg* W </span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">  loss = <span class="number">0.0</span></span><br><span class="line">  dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  fxw = np.dot(X,W) </span><br><span class="line">  fxw = fxw - np.max(fxw,axis = <span class="number">1</span>).reshape(len(fxw),<span class="number">1</span>)</span><br><span class="line">  loss += np.sum(-fxw[list(range(len(fxw))),y] + np.log(np.sum(np.exp(fxw),axis=<span class="number">1</span>)))</span><br><span class="line">  loss /= X.shape[<span class="number">0</span>] </span><br><span class="line">  loss +=  reg * np.sum(W * W)</span><br><span class="line">  sum_f = np.sum(np.exp(fxw), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">  p = np.exp(fxw)/sum_f</span><br><span class="line">  mask = np.zeros_like(p)</span><br><span class="line">  mask[np.arange(X.shape[<span class="number">0</span>]), y] = <span class="number">1</span></span><br><span class="line">  dW = X.T.dot(p - mask)</span><br><span class="line">  dW = dW/X.shape[<span class="number">0</span>]  + <span class="number">2</span>*reg* W </span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Chen Zhou
  </li>

  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/12/MPI_HelloWorld/" rel="next" title="MPI Hello world">
                <i class="fa fa-chevron-left"></i> MPI Hello world
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/07/cs231n-backpropagation/" rel="prev" title="cs231n 反向传播推导">
                cs231n 反向传播推导 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Chen Zhou" />
            
              <p class="site-author-name" itemprop="name">Chen Zhou</p>
              <p class="site-description motion-element" itemprop="description">Get busy living, or get busy dying.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/chenzhou9513" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="chenzhou9513@zju.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Softmax-相关推导-cs-231n版本"><span class="nav-number">1.</span> <span class="nav-text">Softmax 相关推导 (cs 231n版本)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-损失函数"><span class="nav-number">1.2.</span> <span class="nav-text">Softmax 损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax损失函数求导"><span class="nav-number">1.3.</span> <span class="nav-text">Softmax损失函数求导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax矩阵形式求导"><span class="nav-number">1.4.</span> <span class="nav-text">Softmax矩阵形式求导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现代码-CS-231n-Assignment-1"><span class="nav-number">1.5.</span> <span class="nav-text">实现代码    CS 231n    Assignment 1</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Zhou</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  







  
  





  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_sphere.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
